{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import data\nimport pandas as pd\nimport cv2\n\ndata = []\n\ncategory = {0: 'cardboard', 1: 'glass', 2: 'metal', 3: 'paper', 4: 'plastic', 5: 'trash'}\n\n# get text data\nwith open (\"/kaggle/input/garbage-classification/zero-indexed-files.txt\", \"r\") as text:\n    data = text.read().splitlines()\n    \n\ndf = pd.DataFrame(columns=['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash'])\n\nfor img_pair in data[:10]:\n    print(img_pair)\n    path = f\"/kaggle/input/garbage-classification/Garbage classification/Garbage classification/{category[img_pair[len(img_pair) - 1]]}/{img_pair[:-2]}\"\n    print(path)\n    img = cv2.imread(path)\n    print(img)\n    \n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install flickrapi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import flickrapi\nimport xml.etree.ElementTree as ET\nimport urllib.request\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# open the .txt list of bird species and read them in as a list \nwith open(\"/kaggle/input/recyclableitemslabels2/recyclable-items version 2.txt\") as recyclable_items_file:\n    recyclable_items = [line.strip() for line in recyclable_items_file]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get an api key and secret from flickr\napi_key = u'150adfe3c2bb6b37e2160d88724730c6'\napi_secret = u'ec7d59b70b6720a6'\nflickr = flickrapi.FlickrAPI(api_key, api_secret)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfor item in recyclable_items:\n    photos = flickr.walk(text=item,\n                        tag_mode='all',\n                        tags=item,\n                        extras='url_c',\n                        privacy_filters = 1,\n                        per_page=500,         \n                        sort='relevance')\n    for i, photo in tqdm(enumerate(photos)):\n        url = photo.get('url_c')\n        #if an error occurs just keep moving\n        try:\n            # first 10 images are for test\n            if i <= 19:\n                TestDirectory = f'/kaggle/working/ITEMS/test/{item}'\n                #check if directory exists\n                if not os.path.exists(TestDirectory):\n                    os.makedirs(TestDirectory)\n                #create file path\n                filepath = os.path.join(TestDirectory, f'{i+1}.jpg')\n                # Download image from the url and save it to '00001.jpg'\n                urllib.request.urlretrieve(url, filepath)\n\n            #next five images are for validation after model has been created\n            elif i < 25 and i > 19:\n                #create a validation folder in another location\n                ValidateDirectory = f'/kaggle/working/Item_Validate/validate/{item}'\n                #check if directory exists\n                if not os.path.exists(ValidateDirectory):\n                    os.makedirs(ValidateDirectory)\n                #create file path\n                filepath = os.path.join(ValidateDirectory, f'{i+1}.jpg')\n                # Download image from the url and save it to '00001.jpg'\n                urllib.request.urlretrieve(url, filepath)\n\n            #stop after 200 total images\n            elif i > 500:\n                break\n\n            #all remaining images go to training\n            else:\n                TrainDirectory = f'/kaggle/working/ITEMS/train/{item}'\n                #check if directory exists\n                if not os.path.exists(TrainDirectory):\n                    os.makedirs(TrainDirectory)\n                #create file path    \n                filepath = os.path.join(TrainDirectory, f'{i-9}.jpg')\n                # Download image from the url and save it to '00001.jpg'\n                urllib.request.urlretrieve(url, filepath)\n        except:\n            pass\nimport shutil\nshutil.make_archive(\"items\", 'zip', \"/kaggle/working/ITEMS\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"validation\", 'zip', \"/kaggle/working/Item_Validate/validate/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.rmtree('/kaggle/working/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Dataset","metadata":{}},{"cell_type":"code","source":"KaggleDatasets().get_gcs_path()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n\nBATCH_SIZE = 1\nIMG_SIZE = (256, 256)\n\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    \"/kaggle/input/ecycle-images/E Cycle Image Data/recyclable items/train\",\n    shuffle=True,\n    batch_size=BATCH_SIZE,\n    image_size=IMG_SIZE\n)\n\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    \"/kaggle/input/ecycle-images/E Cycle Image Data/recyclable items/test\",\n    shuffle=True,\n    batch_size=BATCH_SIZE,\n    image_size=IMG_SIZE\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of training batches: %d' % tf.data.experimental.cardinality(train_dataset).numpy())\nprint('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset).numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nwith tpu_strategy.scope():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(28, kernel_size=(3, 3), input_shape=(256, 256, 3)))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.Flatten())  # Flattening the 2D arrays for fully connected layers\n    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n    model.add(tf.keras.layers.Dropout(0.25))\n    model.add(tf.keras.layers.Dense(28, activation=tf.nn.softmax))\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n: Model Summary\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nmodel.compile(optimizer='SGD', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset, epochs=10 ,validation_data=(test_dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing from TensorFlow MNIST\nMARK: https://www.tensorflow.org/datasets/keras_example","metadata":{}},{"cell_type":"code","source":"def normalize_img(image, label):\n  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n  return tf.cast(image, tf.float32) / 255., label\n\nds_train = ds_train.map(\n    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\nds_train = ds_train.cache()\n# ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\nds_train = ds_train.batch(128)\nds_train = ds_train.prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_test = ds_test.map(\n    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\nds_test = ds_test.batch(8)\nds_test = ds_test.cache()\nds_test = ds_test.prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dense(10)\n])\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n)\n\ntqdm(model.fit(\n    ds_train,\n    epochs=2,\n    validation_data=ds_test,\n))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Code from TDS\n\nMARK: https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428","metadata":{}},{"cell_type":"code","source":"EPOCHS = 32\niter = train_dataset.makeOneShotIterator()\nx, y = iter.get_next()\n# make a simple model\nnet = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\nnet = tf.layers.dense(net, 8, activation=tf.tanh)\nprediction = tf.layers.dense(net, 1, activation=tf.tanh)\nloss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\ntrain_op = tf.train.AdamOptimizer().minimize(loss)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(EPOCHS):\n        _, loss_value = sess.run([train_op, loss])\n        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_dataset.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\nds_numpy = tfds.as_numpy(train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}